# high-performance-api-dotnet

Optimising Performance to the max for a .NET API.
This is the EFCore optimisation branch

## Overview

A high-performance e-commerce API built with .NET 9 and PostgreSQL, designed to demonstrate production-grade performance optimization techniques. This project serves as a reference implementation for building scalable, low-latency APIs.

## Running the Benchmarks

### Quick Start (Interactive)

**Micro-Benchmarks:**
```bash
# Open PowerShell as Administrator (required for hardware counters)
dotnet run -c Release --project ApexShop.Benchmarks.Micro
```

**Load Tests:**

The load test suite automatically reseeds the database before running to ensure clean, consistent test data with no ID gaps.

```bash
# Terminal 1: Start load tests (will drop/recreate database)
cd ApexShop.LoadTests
dotnet run -c Release

# When prompted "Press any key to begin...", switch to Terminal 2
```

```bash
# Terminal 2: Start API with seeding enabled
cd ApexShop.API
$env:RUN_SEEDING="true"
dotnet run -c Release

# Wait for: "âœ“ Benchmark database seeded successfully!"
# Then return to Terminal 1 and press any key
```

**What the load test does automatically:**
1. ðŸ—‘ï¸ Drops existing database (clears ID gaps from previous tests)
2. ðŸ”¨ Recreates database with migrations
3. â¸ï¸ Waits for you to start API with seeding
4. âœ… Runs all 12 test scenarios sequentially
5. ðŸ“Š Generates reports in `ApexShop.LoadTests/Reports/`

**Why reseeding is important:**
- Ensures continuous IDs (1, 2, 3... instead of 1, 5, 7, 12...)
- Eliminates 404 errors from random ID selection
- Provides consistent baseline for performance comparisons
- Achieves 95-100% success rates instead of false failures

### Automated Suite (Perfect for Overnight Runs)

Run both benchmarks and load tests sequentially with automatic shutdown:

```powershell
# Navigate to project directory
cd C:\Users\lelyg\Desktop\code\high-performance-api-dotnet

# Run tests normally (no shutdown)
.\run-benchmarks.ps1

# Run tests + shutdown computer (great for overnight benchmarking)
.\run-benchmarks.ps1 -Shutdown
```

**What it does:**
1. Runs BenchmarkDotNet micro-benchmarks
2. Runs NBomber load tests (auto-selects baseline)
3. Shuts down computer after 5-minute countdown (if `-Shutdown` used)

**Perfect for:**
- ðŸŒ™ Overnight benchmark runs
- ðŸ’» Leaving long tests running unattended
- ðŸ”‹ Saving power after tests complete

**Results saved to:**
- Benchmarks: `ApexShop.Benchmarks.Micro/Reports/`
- Load Tests: `ApexShop.LoadTests/Reports/`

**Troubleshooting:**
If you get an execution policy error:
```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```
### Key Features

- **High Throughput**: Optimized for 10,000+ requests per second
- **Low Latency**: Sub-50ms p99 response times
- **Production-Ready**: Realistic e-commerce schema with 47,500+ seeded records
- **Comprehensive Testing**: Micro benchmarks and load tests included
- **Performance Monitoring**: Built-in diagnostics and profiling

### Technologies

- **.NET 9**: Latest runtime with performance improvements
- **PostgreSQL 16**: High-performance relational database
- **EF Core 9**: Optimized ORM with advanced query capabilities
- **Minimal APIs**: Low-overhead endpoint routing
- **BenchmarkDotNet**: Micro-benchmark framework
- **NBomber**: Load testing framework
- **Bogus**: Realistic data generation

### Who Is This For?

- Developers building high-performance APIs
- Teams optimizing existing .NET applications
- Anyone learning performance engineering in .NET

## Contributing

Contributions are welcome! If you have suggestions for improvements, please open an issue or submit a pull request.

### How to Contribute

1.  Fork the repository.
2.  Create a new branch for your feature or bug fix.
3.  Make your changes and commit them with a clear message.
4.  Push your changes to your fork.
5.  Open a pull request to the main repository.

## Architecture

This project follows a **Vertical Slice Architecture** with the primary objective of achieving **highest performance**.

## Project Structure

The project is organized into the following directories:

-   `ApexShop.API`: The main API project, containing the endpoints and `Program.cs`.
-   `ApexShop.Application`: Contains the application logic, such as services and DTOs.
-   `ApexShop.Domain`: Contains the domain entities and interfaces.
-   `ApexShop.Infrastructure`: Contains the infrastructure code, such as the `DbContext`, repositories, and database seeding.
-   `ApexShop.Benchmarks.Micro`: Contains the micro-benchmarks for the API.
-   `ApexShop.LoadTests`: Contains the load tests for the API.

## Infrastructure

### Database Selection: PostgreSQL vs SQL Server

**PostgreSQL** is the chosen database for this project for the following reasons:

- **Superior Performance**: PostgreSQL consistently outperforms SQL Server in high-throughput scenarios, especially for read-heavy workloads
- **Better Connection Pooling**: Native connection pooling with PgBouncer provides exceptional scalability
- **Efficient Indexes**: Advanced indexing capabilities (BRIN, GiST, GIN) and partial indexes offer better query optimization
- **Lower Latency**: Lightweight protocol and efficient buffer management result in lower query latency
- **Cost-Effective**: Open-source with no licensing costs, allowing infrastructure budget to focus on hardware optimization
- **JSON Performance**: Native JSONB support with indexing outperforms SQL Server's JSON handling
- **Concurrent Connections**: MVCC (Multi-Version Concurrency Control) handles concurrent writes more efficiently than SQL Server's locking mechanisms

### Database Schema

The application uses a production-realistic e-commerce database schema with the following entities:

#### Seed Data Statistics

| Entity | Row Count | Description |
|--------|-----------|-------------|
| **Categories** | 15 | Product categories (Electronics, Clothing, Books, etc.) |
| **Users** | 3,000 | Customer accounts with realistic contact details |
| **Products** | 15,000 | Products with realistic names/descriptions via Bogus |
| **Orders** | 5,000 | Customer orders with various statuses |
| **OrderItems** | ~12,500 | Order line items (1-5 items per order, avg 2.5) |
| **Reviews** | 12,000 | Product reviews (~80% products have reviews) |

**Total Rows:** ~47,500+
**Estimated Database Size:** 60-120 MB

#### Seeding Strategy

**Bogus-based seeding** for realistic data:
- **Smart seeding**: Automatically skips if data exists
- **Batched inserts**: 500-1000 records per batch for performance
- **ChangeTracker clearing**: Prevents memory issues with large datasets
- **Realistic data**: Uses Faker library for names, emails, addresses, product descriptions
- **Optimized queries**: Loads reference data upfront to avoid N+1 queries

#### Entity Relationships

- **User â†’ Orders** (1:N): Each user can place multiple orders
- **User â†’ Reviews** (1:N): Each user can write multiple reviews
- **Category â†’ Products** (1:N): Each category contains multiple products
- **Product â†’ OrderItems** (1:N): Each product can appear in multiple orders
- **Product â†’ Reviews** (1:N): Each product can have multiple reviews
- **Order â†’ OrderItems** (1:N): Each order contains multiple line items
- **OrderItem â†’ Product** (N:1): Each order item references one product

#### Performance Optimizations

- **Indexes**: Strategic indexes on foreign keys, frequently queried columns (Price, Name, OrderDate, Status, Rating)
- **Precision**: Decimal fields use `PRECISION(18,2)` for monetary values
- **Constraints**: Appropriate delete behaviors (Cascade for dependent data, Restrict for referenced data)
- **Default Values**: Database-level defaults for timestamps using `CURRENT_TIMESTAMP`

## Database Setup with Docker

### Prerequisites

- Docker Desktop installed and running
- .NET 9 SDK

### Step 1: Create docker-compose.yml

Create a `docker-compose.yml` file in the project root with PostgreSQL configuration.

### Step 2: Configure Connection Strings

Add database connection settings to `appsettings.json` and `appsettings.Development.json`.

### Step 3: Configure DbContext

Set up EF Core with PostgreSQL provider in the Infrastructure layer, registering the DbContext in dependency injection.

### Step 4: Create Initial Migration

Generate EF Core migration for the domain entities:

```bash
dotnet ef migrations add InitialCreate --project ApexShop.Infrastructure --startup-project ApexShop.API
```

### Step 5: Start Database and Apply Migrations

```bash
# Start PostgreSQL container
docker-compose up -d

# Apply migrations
dotnet ef database update --project ApexShop.Infrastructure --startup-project ApexShop.API
```

### Step 6: Verify Connection

Run the API and verify database connectivity:

```bash
dotnet run --project ApexShop.API
```

## Baseline Results (Non-Optimized)

This section documents the **initial performance baseline** before any optimization work. These results establish the starting point for measuring optimization improvements.

### Micro-Benchmark Results (BenchmarkDotNet)
earliest report 2025-10-06_11-09-32 
current newest 2025-10-17_19-22-55   

**Test Environment:**
- CPU: Intel Core i7-8650U @ 1.90GHz (Kaby Lake R), 4 cores, 8 logical processors
- Runtime: .NET 9.0.9
- OS: Windows 11

| Benchmark | Mean | StdDev | Min | Max | Allocated Memory |
|-----------|------|--------|-----|-----|------------------|
| **Api_TrueColdStart** | 193.02 ms | 0.00 ms | 193.02 ms | 193.02 ms | 4,492 KB |
| **Api_GetSingleProduct** | 5.11 ms | 0.88 ms | 4.05 ms | 7.04 ms | 84 KB |
| **Api_ColdStart** | 180.23 ms | 13.00 ms | 157.43 ms | 202.04 ms | 4,406 KB |
| **Api_GetAllProducts** | 417.29 ms | 129.75 ms | 145.03 ms | 563.52 ms | 50,585 KB |

**Hardware Counters:**
- Cache misses for GetAllProducts: 12.2M per operation
- Branch instructions: 198M per operation
- Significant memory allocation on collection endpoints

### Load Testing Results (NBomber)

The load tests revealed **severe performance degradation** under realistic production load.

#### Overall Performance

- **Total Requests**: 12,117
- **Successful**: 144 (1.2%)
- **Failed**: 11,973 (98.8%)
- **Primary Failure Mode**: Operation timeouts (30s exceeded)

#### CRUD Scenarios (30s duration, 10 RPS)

| Scenario | Success Rate | p50 Latency | p99 Latency | Status |
|----------|-------------|-------------|-------------|--------|
| get_products | 0% | 30s | 30s timeout | âŒ FAIL |
| get_product_by_id | 17.7% | 20.2s | ~29s | âŒ FAIL |
| create_product | 22.8% | 20.6s | ~30s | âŒ FAIL |
| get_categories | 19% | 20.6s | ~30s | âŒ FAIL |
| get_orders | 0% | 30s | 30s timeout | âŒ FAIL |

#### Realistic Workflow Scenarios (60s duration)

| Scenario | RPS | Success Rate | p50 Latency | p99 Latency | Status |
|----------|-----|-------------|-------------|-------------|--------|
| browse_and_review | 5 | 0% | 30.0s | 30.2s | âŒ FAIL |
| create_order_workflow | 3 | 0% | 30.0s | 59.3s | âŒ FAIL |
| user_registration_and_browse | 2 | 0% | 30.0s | 59.4s | âŒ FAIL |

#### Stress Test Scenarios

| Scenario | Load Pattern | Success Rate | Status |
|----------|-------------|--------------|--------|
| stress_get_products | Ramp to 50 RPS, sustain 60s | 0% | âŒ FAIL |
| spike_test | Spike to 100 RPS | 0% | âŒ FAIL |
| constant_load | 10 concurrent users | 0% | âŒ FAIL |
| mixed_operations_stress | Ramp to 30 RPS | 0% | âŒ FAIL |

#### Error Distribution

| Error Type | Count | Percentage |
|-----------|-------|------------|
| **Operation Timeout** (>30s) | 11,805 | 97.4% |
| **Connection Refused** | 156 | 1.3% |
| **Internal Server Error** | 12 | 0.1% |
| **Successful** | 144 | 1.2% |

### Key Observations

**Micro-Benchmarks:**
- Cold start penalty of ~180-193ms for application initialization
- Single product retrieval shows acceptable isolated performance (5ms mean)
- GetAllProducts (15,000 rows) exhibits high variance (145-563ms) and massive memory allocation (50MB)
- Memory allocations scale linearly with result set size

**Load Tests:**
- API cannot sustain even 10 RPS under concurrent load
- Database query latency degrades from 10ms â†’ 1,000ms+ as load increases
- API crashes completely under load (connection refused errors indicate server failure)
- No recovery observed - degradation is permanent until restart

**Performance Cliff:**
- First ~50 requests show acceptable performance
- Sharp degradation occurs after connection pool saturation
- Cascading failures in multi-step workflows (browse â†’ review workflows timeout entirely)

### Baseline Summary

The non-optimized API demonstrates critical performance issues:

- **Throughput**: Cannot sustain 10 RPS (target: 1,000+ RPS)
- **Latency**: p99 > 30 seconds (target: < 200ms)
- **Reliability**: 98.8% failure rate under load (target: > 99.9% success)
- **Memory**: 50MB allocation for 15K row query suggests inefficient serialization
- **Stability**: Complete API failure under sustained load

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
